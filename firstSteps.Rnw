\documentclass[11pt]{article}
%\usepackage[showframe]{geometry}
\usepackage{caption}
\usepackage{lscape,verbatim,mathrsfs}
\usepackage{graphics,amsmath,pstricks}
\usepackage{amssymb,enumerate}
\usepackage{amsbsy,amsmath,amsthm,amsfonts, amssymb}
\usepackage{graphicx, rotate, array}
\usepackage{geometry,multirow}
\usepackage{float}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
%\renewcommand{\baselinestretch}{1.9}

\renewcommand{\familydefault}{cmss}
\textwidth=6.65in \textheight=9.7in
\parskip=.025in
\parindent=0in
\oddsidemargin=-0.1in \evensidemargin=-.1in \headheight=-.6in
\footskip=0.5in \DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\SweaveOpts{concordance=TRUE}

% Preview source code from paragraph 0 to 10


\title{Base Case TMLE Procedure}
\maketitle

\section*{Step 1: Simulate from above, 10 time points}

Assume the end time, $\tau=10$ and we start with binaries $W(0),A(0),Y(0).$
From there,

$W(t)$ depends on $C_w(t) = (Y(t-1),A(t-1))$

$A(t)$ depends on $C_a(t) = (W(t),Y(t-1))$ 

$Y(t)$ depends on $C_y(t) = (A(t),W(t))$ 

\section*{Step 2: Estimate the W,A,Y mechanisms which determine the likelihood}

$g^{*}(A(1)=1\mid W)=1$ is the intervention, leaving everything else
in tact. 

First we use the data to estimate coefficients of previous variables
so we have $Pr(Y(t)=1)=expit(\alpha_{0}+\alpha_{1}A(t)+\alpha_{2}W(t))$
estimates and likewise for $W(t)$ and $A(t)$. 

\section*{Step 3: Compute the estimate}

Perform monte-carlo sampling on the initial estimates to estimate
$EY_{g^{*}}$.  Let us a assume we have fit the following functions from Step 2:

$W(t) = rbern(f_w(C_w(t))$

$A(t) = rbern(f_a(C_a(t))$

$Y(t) = rbern(f_y(C_y(t))$

\begin{enumerate}
\item sample W(1) as a random bernouilli based on $C_w(1)$
\item Interve on A(1) to set it to 1 (for the remaining time points we will sample a random bernoulli based on $C_a(t)$)
\item sample Y(1) as a random bernouilli based on $C_y(1)$ 
\item repeat steps 1 to 3 until you sample the outcome of interest, $Y_{g^*}(100)$
\item repeat steps 1 to 4 many times and average to get the mean outcome under the current estimated model
\end{enumerate}

\section*{Step 4: Compute the EIC and Clever Covariate}

Now we will refer to chapter 19 in the new book on tstmle github. 

\subsection*{computing the clever covariate for the $\bar{Q}_{y}$ update as well
as IC}

First, this component of the influence curve is given by $\frac{1}{N}\sum_{i=1}^{n}\bar{D}_{\bar{q}_{y}}(Y(i),C_{y}(i))=\frac{1}{N}\sum_{i=1}^{n}H_{y}(C_{y}(i))(Y(i)-\bar{Q}_{y}(1,C_{y}(i)))$. 

Note that: 

$C_{y}(i)=(A(i),W(i))$,

$C_{a}(i)=(W(i),Y(i-1))$ 

$C_{w}(i)=(Y(i-1),A(i-1))$

Notice that the key here is to compute $H_{y}(C_{y}(i))$ because
this will be our typical clever covariate for the $i^{th}$ time point
for dependent observation, $Y(i).$ 

In a loop for i in 1:N

for a nested loop from s in 1:N 

compute 

$H_{y(s)}(C_{y}(i))=\mathbb{E}[Y_{g^{*}}\mid Y(s)=1,C_{y}(s)=C_{y}(i)]-\mathbb{E}[Y_{g^{*}}\mid Y(s)=0,C_{y}(s)=C_{y}(i)]$ 

This will take montecarlo sampling using the binaries already estimated
in step 2. This will be the same as outlined in Step 3, except for each expectation, start with generating $W(s+1)$, then $A(s+1)$ etc until the outcome at time, $\tau=100$. Do this
a lot of times. Then average the resulting differences. The clever covariates: $\frac{h_{c_{y}(s)}^{*}(C_{y}(i))}{\bar{h}_{c_{y}}(C_{y}(i))}$ are accomplished via page 327, which entails generating B outcomes
under intervention and not under intervention. Again, we just montecarlo
sample as in step 2, except save the entire observation, $O^N$. We also (as it says in the book) need not apply
logistic regression here as we can do it nonparametrically. Now store
$H_{y(s)}(C_{y}(i))\frac{h_{c_{y}(s)}^{*}(C_{y}(i))}{\bar{h}_{c_{y}}(C_{y}(i))}$.
Concurrently, do the same thing for $H_{g(s)}(C_{a}(i))=\mathbb{E}[Y_{g^{*}}\mid W(s)=1,C_{a}(s)=C_{a}(i)]-\mathbb{E}[Y_{g^{*}}\mid W(s)=0,C_{a}(s)=C_{a}(i)]$
and $\frac{h_{c_{y}(s)}^{*}(C_{a}(i))}{\bar{h}_{c_{y}}(C_{a}(i))}$,
except only do so on non-intervention nodes as there is no such component
in the IC for intervention nodes as usual. On intervention nodes $H_{a(s)}(C_{a}(i))\frac{h_{c_{a}(s)}^{*}(C_{a}(i))}{\bar{h}_{c_{a}}(C_{a}(i))}=0$.
Do likewise for $w$ on all nodes. 

end the inner loop

store $H_{y}(C_{y}(i))$, $H_{a}(C_{a}(i))$ and $H_{w}(C_{w}(i))$ 

store $\bar{D}^{N}(O(i))=H_{y}(C_{y}(i))(Y(i)-\bar{Q}_{y}(1\mid C_{y}(i))+H_{a}(C_{a}(i))$$(A(i)-\bar{g}(1\mid C_{a}(i))+H_{w}(C_{w}(i))(W(i)-\bar{q}_{w}(1\mid C_{w}(i))$
where $\bar{Q}_{y},\bar{g}$ and $q_{w}$ are the initial estimates.

end outer loop

\section*{Step 4: check tolerance}

compute the sample mean of $\bar{D}^{N}(O)$ and see if below $\frac{\hat{\sigma}}{N}$.
If so, your last estimate is used as well as the influence curve $\bar{D}^{N}(O(i))$
to get inference. Otherwise go to Step 5. We note, that $hat{\sigma}$ is just the sample standard deviation of the influence curve we computed. Each term of the influence curve, $\bar{D}^N(\bar{O}_i)$, has mean 0, conditioned on the past, $\bar{O}_{i-1}$ so we just need sum the squares of each term as an estimate of the conditional variance as given by the Martingale Central Limit Theorem.  

\section*{Step 5: Run the TMLE and compute estimate}

Create a 3N-1 length vector with all the clever covariates and use
as outcome, W(t), A(t) and Y(t). The offset is the corresponding initial
estimate (each is one of 4 binomial probabilities, considering they
are functions of two binaries) . Note that for the intervention nodes
there is no outcome and clever covariate. Use logistic regression
to find $\epsilon_{n}$, our fluctuation parameter. Update as usual
and return to step 1. 
\end{document}