\documentclass[11pt]{article}
%\usepackage[showframe]{geometry}
\usepackage{caption}
\usepackage{lscape,verbatim,mathrsfs}
\usepackage{graphics,amsmath,pstricks}
\usepackage{amssymb,enumerate}
\usepackage{amsbsy,amsmath,amsthm,amsfonts, amssymb}
\usepackage{graphicx, rotate, array}
\usepackage{geometry,multirow}
\usepackage{float}
%\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
%\renewcommand{\baselinestretch}{1.9}

\renewcommand{\familydefault}{cmss}
\textwidth=6.65in \textheight=9.7in
\parskip=.025in
\parindent=0in
\oddsidemargin=-0.1in \evensidemargin=-.1in \headheight=-.6in
\footskip=0.5in \DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\begin{document}
\SweaveOpts{concordance=TRUE}

% Preview source code from paragraph 0 to 10


\title{Base Case TMLE Procedure}
\maketitle

\section*{Step 1: Simulate from above, 10 time points}

Assume the end time, $\tau=10$ and we start with binaries $W(0),A(0),Y(0).$
From there,

$W(t)$ depends on $Y(t-1),A(t-1)$ 

$A(t)$ depends on $W(t),Y(t-1)$ 

$Y(t)$ depends on $(A(t),W(t))$ 

\section*{Step 2: Estimate the W,A,Y mechanisms which determine the likelihood}

$g^{*}(A(1)=1\mid W)=1$ is the intervention, leaving everything else
in tact. 

First we use the data to estimate coefficients of previous variables
so we have $Pr(Y(t)=1)=expit(\alpha_{0}+\alpha_{1}A(t)+\alpha_{2}W(t))$
estimates and likewise for $W(t)$ and $A(t)$. 

\section*{Step 3: Compute the estimate}

Perform monte-carlo sampling on the initial estimates to estimate
$EY_{g^{*}}$.
\begin{enumerate}
\item sample a W(1) by choosing a random binomial based on the past, 
\item Interve on A(1) to set it to 1, 
\item Continue with random binomials until the outcome, according to the
initial estimates.
\item repeat steps 1 to 3 many times and average to get the estimate
\end{enumerate}

\section*{Step 4: Compute the EIC and Clever Covariate}

Now we will refer to chapter 19 in the new book on tstmle github. 

\subsection*{computing the clever covariate for the $\bar{Q}_{y}$ update as well
as IC}

First, this component of the influence curve is given by $\frac{1}{N}\sum_{i=1}^{n}\bar{D}_{\bar{q}_{y}}(Y(i),C_{y}(i))=\frac{1}{N}\sum_{i=1}^{n}H_{y}(C_{y}(i))(Y(i)-\bar{Q}_{y}(1,C_{y}(i)))$. 

Note that: 

$C_{y}(i)=(A(i),W(i))$,

$C_{a}(i)=(W(i),Y(i-1))$ 

$C_{w}(i)=(Y(i-1),A(i-1))$

Notice that the key here is to compute $H_{y}(C_{y}(i))$ because
this will be our typical clever covariate for the $i^{th}$ time point
for dependent observation, $Y(i).$ 

In a loop for i in 1:N

for a nested loop from s in 1:N 

compute 

$H_{y(s)}(C_{y}(i))=\mathbb{E}[Y_{g^{*}}\mid Y(s)=1,C_{y}(s)=C_{y}(i)]-\mathbb{E}[Y_{g^{*}}\mid Y(s)=0,C_{y}(s)=C_{y}(i)]$ 

This will take montecarlo sampling using the binaries already estimated
in step 2. For each expectation, start with generating $W(s+1)$,
then $A(s+1)$ etc until the outcome at time, $\tau=100$. Do this
a lot of times (start with maybe 1000 if it takes a while). Then average
the resulting differences. The clever covariates: $\frac{h_{c_{y}(s)}^{*}(C_{y}(i))}{\bar{h}_{c_{y}}(C_{y}(i))}$
are accomplished via page 327, which entails generating B outcomes
under intervention and not under intervention. Again, we just montecarlo
sample to generate these observations, given our previous estimate
of the distribution. We also (as it says in the book) need not apply
logistic regression here as we can do it nonparametrically. Now store
$H_{y(s)}(C_{y}(i))\frac{h_{c_{y}(s)}^{*}(C_{y}(i))}{\bar{h}_{c_{y}}(C_{y}(i))}$.
Concurrently, do the same thing for $H_{g(s)}(C_{a}(i))=\mathbb{E}[Y_{g^{*}}\mid W(s)=1,C_{a}(s)=C_{a}(i)]-\mathbb{E}[Y_{g^{*}}\mid W(s)=0,C_{a}(s)=C_{a}(i)]$
and $\frac{h_{c_{y}(s)}^{*}(C_{a}(i))}{\bar{h}_{c_{y}}(C_{a}(i))}$,
except only do so on non-intervention nodes as there is no such component
in the IC for intervention nodes as usual. On intervention nodes $H_{a(s)}(C_{a}(i))\frac{h_{c_{a}(s)}^{*}(C_{a}(i))}{\bar{h}_{c_{a}}(C_{a}(i))}=0$.
Do likewise for $w$ on all nodes. 

end the inner loop

store $H_{y}(C_{y}(i))$, $H_{a}(C_{a}(i))$ and $H_{w}(C_{w}(i))$ 

store $\bar{D}^{N}(O(i))=H_{y}(C_{y}(i))(Y(i)-\bar{Q}_{y}(1\mid C_{y}(i))+H_{a}(C_{a}(i))$$(A(i)-\bar{g}(1\mid C_{a}(i))+H_{w}(C_{w}(i))(W(i)-\bar{q}_{w}(1\mid C_{w}(i))$
where $\bar{Q}_{y},\bar{g}$ and $q_{w}$ are the initial estimates.

end outer loop

\section*{Step 4: check tolerance}

compute the sample mean of $\bar{D}^{N}(O)$ and see if below $\frac{\sigma}{N}$.
If so, your last estimate is used as well as the influence curve $\bar{D}^{N}(O(i))$
to get inference. Otherwise go to Step 5. 

\section*{Step 5: Run the TMLE and compute estimate}

Create a 3N-1 length vector with all the clever covariates and use
as outcome, W(t), A(t) and Y(t). The offset is the corresponding initial
estimate (each is one of 4 binomial probabilities, considering they
are functions of two binaries) . Note that for the intervention nodes
there is no outcome and clever covariate. Use logistic regression
to find $\epsilon_{n}$, our fluctuation parameter. Update as usual
and return to step 1. 
\end{document}